"""Evaluation script for the TelecomPlus support agent.

This script evaluates the agent's responses against the expected answers
from the evaluation dataset.

Students should modify this script to:
1. Call their actual agent implementation
2. Implement proper evaluation logic (e.g., LLM-as-a-judge)
3. Calculate meaningful metrics (accuracy, relevance, etc.)
"""

import random

import pandas as pd

from src.main import answer


def evaluate_response(question: str, expected_answer: str, agent_answer: str) -> int:
    """Evaluate the relevance of the agent's answer.

    TODO: Students should implement actual evaluation logic here.
    Consider using:
    - LLM-as-a-judge (e.g., with OpenAI/Anthropic API)
    - Semantic similarity (e.g., with sentence transformers)
    - Custom scoring based on key information extraction

    Args:
        question: The customer question
        expected_answer: The expected answer from the dataset
        agent_answer: The answer generated by your agent

    Returns:
        Score: 1 if relevant, 0 if not (placeholder logic)
    """
    # PLACEHOLDER: Random fake score for demonstration
    # Students should replace this with actual evaluation logic
    return random.choice([0, 1])


def run_evaluation():
    """Run evaluation on all questions from the evaluation dataset."""

    # Load evaluation questions
    print("Loading evaluation questions...")
    df = pd.read_excel("data/evaluation_questions.xlsx")

    print(f"Loaded {len(df)} questions\n")
    print("=" * 80)

    results = []
    total_score = 0

    # Evaluate each question
    for idx, row in df.iterrows():
        question = row["Question"]
        expected_answer = row["Réponse Attendue"]
        difficulty = row["Difficulté"]

        print(f"\nQuestion {idx + 1}/{len(df)} [{difficulty}]:")
        print(f"Q: {question}")

        # Get agent's answer
        # TODO: Students should call their actual agent here
        agent_answer = answer(question)
        print(f"Agent: {agent_answer}")

        # Evaluate the response
        score = evaluate_response(question, expected_answer, agent_answer)
        total_score += score

        print(f"Score: {score}/1")

        # Store results
        results.append({
            "Question": question,
            "Expected Answer": expected_answer,
            "Agent Answer": agent_answer,
            "Difficulty": difficulty,
            "Score": score
        })

        print("-" * 80)

    # Calculate final metrics
    accuracy = total_score / len(df)
    print("\n" + "=" * 80)
    print("EVALUATION RESULTS")
    print("=" * 80)
    print(f"Total questions: {len(df)}")
    print(f"Total score: {total_score}/{len(df)}")
    print(f"Accuracy: {accuracy:.2%}")

    # Save results to Excel
    results_df = pd.DataFrame(results)
    results_df.to_excel("evaluation_results.xlsx", index=False)
    print("\nResults saved to: evaluation_results.xlsx")


if __name__ == "__main__":
    run_evaluation()
